\chapter{Fundamental Machine Learning Concepts}
\section{Introduction}

In this thesis we explore the application of advanced machine learning methods to experimental nuclear physics data. To properly understand the framework of optimization, validation and challenges we face we'll introduce them in turn using a couple of simple models; linear and logistic regression. Before those discussions we briefly outline the process of model fitting and briefly introduce the difference in models with a known or unknown outcome.

Fitting models to data is the formal framework by which much of modern science is underpinned. In most sciences the researcher has a need to formulate some model that represents a given theory. In physics we construct models to describe complex natural phenomena which we use to make predictions or infer inherent properties about the natural world. These models vary from the estimating the Hamiltonian of a simple binary spin system like the ising model, to more complex methods like variational markov chain monte-carlo which is used for many-body quantum mechanics.

We view this process as approximating an unknown function $\hat{f}$ which takes a state $\mathbf{X}$Â as input and gives some output $\mathbf{\hat{y}}$,  
 $$\hat{f}(\mathbf{X}) = \mathbf{\hat{y}}.$$ 
To approximate this function we use an instance of a model: $f(\mathbf{X}; \theta) = \mathbf{y}$, where we don't necessarily have a good ansatz for the form of $f$ or the parameters $\theta$. Additionally the output of the function can be multi-variate, discrete or continuous which informs the choice of model for a particular problem. In this first part of the chapter we consider a single real valued outcome. Additionally we note  that in this thesis we use a notation on the form $f(y | x; \theta )$ which reads as the function $f$ with output $y$ given $x$ and the parameters $\theta$, and is equivalent to the notation $y = f(x; \theta)$.

 Returning to the question of approximating $\hat{f}$ we begin with the objective of the model: to minimize the discrepancy, $g(|\hat{y} - y|)$, between our approximation and the true target values. An  example of the function $g$ is the mean squared error function used in many modeling applications, notably in linear regression which we explore in section \ref{sec:LinReg}.

In this paradigm we have access to the outcomes of our process, $\hat{y}$, and the system states, $\mathbf{X}$. However this thesis deals largely with the problem of modeling when one only has access to the system states.  However the concepts, terminology and challenges inherent to the former are also ones we have to be mindful of in the latter.

Approximating functions with access to process outcomes starts with the separation of our data into two sets with zero intersection. This is done such that we are able to estimate the performance of our model in the real world. To elaborate the need for this separation we explore the concepts of overfitting and underfitting to the data later in this chapter.