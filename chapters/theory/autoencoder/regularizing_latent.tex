\subsection{Regularizing Latent Spaces}\label{sec:latent}

As introduced in section \ref{sec:vae} the latent space of an autoencoder can be regularized to satisfy some distribution. The nature and objective of this regularization has been the subject of debate in the machine learning literature since Kingma's original VAE paper in 2014 (\cite{Kingma2013}). Two of the seminal papers published on the topic is the $\beta-VAE$ paper by \citet{Higgins2017} introducing a weight on the traditional KL divergence term, and the Info-VAE paper by \citet{Zhao} criticizing the choice of a KL-divergence on the latent space. Where they further build on the $\beta-VAE$ proposition that the reconstruction and latent losses are not well balanced, and show that one can replace the KL-divergence term with another strict divergence and empirically show better results with these. In particular they show strong performance with a Maximum-Mean Discrepancy (MMD) divergence, using any positive definite kernel $k(\cdot, \cdot)$\footnote{We will not probe deeply into the mathematics of kernel functions but they are used in machine learning most often for measuring distances, or applications in finding nearest neighbors. They are ordinarily proper metric functions. Some examples include the linear kernel: $k(x, x') = x^Tx'$ or the popular radial basis function kernel $k(x, x)=e^{-\frac{||x - x'||^2}{2\sigma}}$} it is defined as:  

\begin{align}
D_{MMD} = \langle k(z, z')\rangle_{p(z), p(z')} - 2 \langle k(z, z')\rangle_{q(z), p(z')} + \langle k(z, z')\rangle_{q(z), q(z')}
\end{align}