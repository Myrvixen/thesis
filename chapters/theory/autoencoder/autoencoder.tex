% !TEX spellckeck=en_GB

\section{Autoencoders}\label{sec:autoencoder}


\subsection{Introduction to autoencoders}\label{sec:intro_autoenc}
An Autoencoder is an attempt at learning a distribution over some data by reconstruction. The interesting part of the algorithm is in many applications that it is in the family of latent variable models. Which is to say the model encodes the data into a lower dimensional latent space before reconstruction. The goal, of course, is to learn the distribution $P(\mathcal{X})$ over the data with some parametrized model $Q(\mathcal{X}|\theta)$
The model consists of two discrete parts ; an encoder and a decoder.
Where the encoder is in general a non linear map $\psi$. 

\begin{align*}
  \psi: \mathcal{X} \rightarrow \mathcal{Z}.
\end{align*}

\noindent Where $\mathcal{X} $ and $\mathcal{Z}$ are arbitrary vector spaces with $\text{dim}(\mathcal{X}) > \text{dim}(\mathcal{Z})$.
The second part of the model is the decoder that maps back to the original space.


\begin{align*}
  \phi: \mathcal{Z} \rightarrow \mathcal{X}.
\end{align*}

\noindent The objective is then to find the configuration of the two maps $\phi$ and $\psi$
that gives the best possible reconstruction, i.e the objective $\mathcal{O}$ is given as

\begin{align}\label{eq:objective_autoenc}
  \mathcal{O} = \argmin_{\phi, \psi} || X - \phi \circ \psi||^2.
\end{align}

\noindent  Where the $\circ$ operator denotes function composition in the standard manner. As the name implies the encoder creates a lower-dimensional "encoded" representation of the input. This objective function is optimized by a mean-squared-error cost in the event of real valued data, but more commonly through a binary crossentropy for data normalized to the range $[0, 1]$.
This representation can be useful for identifying whatever information-carrying variations are present in the data. This can be thought of as an analogue to Principal Component Analysis (PCA) (\cite{Marsland2009}). In practice the non-linear maps, $\psi$ and $\phi$, are most often parametrized and optimized as ANNs. ANNs are described in detail in section \ref{sec:ANN}. The autoencoder was used perhaps most successfully in a de-noising tasks. \todo{Citation needed. Also should I include example of denoising autoencoders ? Maybe a description at least.. Link to notebook maybe? }
More recently the Machine Learning community discovered that one could impose a regularizing divergence term on the latent distribution allow for the imposition of structure in the latent space. The first of these employed a Kullback-Leibler divergence and was dubbed "Variational Autoencoders", or VAEs, (\cite{Kingma2013}). While VAEs lost the contest for preeminence as a generative algorithm to adversarial networks they remain a fixture in the literature of expressive latent variable models with development focusing on the expressibility of the latent space (\todo{citation InfoVAE and $\beta$-VAE}).

\subsection{Variational Autoencoder}\label{sec:vae}

Originally presented by \citet{Kingma2013} the Variational Autoencoder (VAE) is a twist upon the traditional autoencoder. Where the applications of an ordinary autoencoder largely extended to de-noising with some authors using it for dimensionality reduction before training an ANN on the output the VAE seeks to control the latent space of the model. The goal is to be able to generate samples from the unknown distribution over the data. Imagine trying to draw a sample from the distribution of houses, we'd be hard pressed to produce anything remotely useful but this is the goal of the VAE. In this thesis the generative properties of the algorithm is only interesting as a way of describing the latent space. Our efforts largely concentrate on the latent space itself and importantly discerning whether class membership, be it a physical property or something more abstract \footnote{examples include discerning whether a particle is  a proton or electron, or capturing the "five-ness" of a number in the MNIST dataset}xw is encoded.

\subsubsection*{The variational autoencoder cost}

In section \ref{sec:intro_autoenc} we presented the structure of the autoencoder rather loosely. For the VAE which is a more integral part of the technology used in the thesis a more rigorous approach is warranted. We will here derive the loss function for the VAE in such a way that makes clear how we aim to impose known structure of the latent space. We begin by considering the family of problems encountered in variational inference, where the VAE takes its theoretical inspiration from. We define the joint probability distribution of some hidden variables $z$ and our data $x$ conditional on some $\beta$. In a traditional modeling context we would coin $z$ as including model parameters and $\beta$ would then denote the hyperparameters. The variational problem is phrased in terms of finding the posterior over $z$, given $\beta$,

\begin{equation}\label{eq:vbayes}
p(z | x, \beta) = \frac{p(z, x|\beta)}{\int_z p (z, x|\beta)}.
\end{equation}

\noindent The integral in the denominator is intractable for most interesting problems \todo{citation?}. This is also the same problem that Markov Chain Monte Carlo (MCMC) methods aim at solving. In physics this family of algorithms has been applied to solve many-body problems in quantum mechanics primarily by gradient descent on variational parameters \todo{citation? Comph-phys 2 compendium?}.

\noindent Next we introduce the Kullback-Leibler divergence (KL-divergence) (\cite{Kullback1951}) which is a measure of how much two distributions are alike, it is important to not that it is however not a metric. We define the KL-divergence in equation \ref{eq:kl} from a probability measure P, to another Q, by their probability density functions p, q over the set $x \in \mathcal{X}$

\begin{align}\label{eq:kl}
D_{KL} (P || Q) &= - \int^{\infty}_{-\infty} p(x) \log \left(\frac{p(x)}{q(x)}\right) dx \\
&= \langle \log \left(\frac{p(x)}{q(x)} \right)\rangle_{p}.
\end{align}

\noindent In the context of the VAE the KL-divergence is a measure of dissimilarity of P approximating Q (\cite{Burnham2002}). The derivation then sensibly starts with a KL-divergence. \\

\noindent We begin by defining $q(z|x)$ to be the true posterior distribution over the latent variable $z \in \mathcal{Z}$, conditional on our data $x \in \mathcal{X}$ with a true posterior distribution $p(x)$ and $q(z)$, with an associated probability measure $Q$ as per our notation above. Let then the distribution over the latent space parametrized by the autoencoder be given as $\psi(z|x)$, where the autoencoder parametrizes a distribution $\eta(x)$, and an associated probability measure $\Psi$. And recalling Bayes rule for conditional probability distributions $p(z | x ) = (p(x | z) p(z) / p(x) $ we then have 

\begin{align}
D_{KL}(\Psi || Q ) &= \langle \log \left(\frac{\psi(z|x)}{q(z|x)}\right) \rangle_\psi \\
&=  \langle \log \left( \psi(z|x)\right) \rangle_\psi - \langle \log \left( p( x | z) q(z) \right) \rangle_\psi + \log \left(p(x) \right) \\
&=  \langle \log \left(\frac{\psi(z|x)}{q(z)} \right) \rangle_\psi - \langle \log \left( p( x | z)\right) \rangle_\psi + \log \left(p(x) \right).
\end{align}

\noindent Note that the term $-\langle \log \left( p( x | z)\right) \rangle_\psi$ is the log likelihood of our decoder network which we can optimize with the cross entropy as discussed in section \ref{sec:LogReg}. Rearranging the terms we arrive at the variational autoencoder cost

\begin{equation}\label{eq:vae_cost}
\log(p(x)) - D_{KL}(\Psi || Q )=  \langle \log \left( p( x | z)\right) \rangle_\psi - \langle \log \left(\frac{\psi(z|x)}{q(z)}\right)\rangle_\psi.
\end{equation}
 

\noindent We are still bound by the intractable integral defining the evidence $p(x) = \int_z p(x, z)$ which is the same integral as in the denominator in equation \ref{eq:vbayes}. The solution appears by approximating the KL-divergence up to an additive constant by estimating the evidence lower bound (ELBO). This function is defined as 

\begin{equation}\label{eq:elbo}
ELBO(q) = \langle \log(p(z, x)) \rangle - \langle \log(q(z|x)) \rangle.
\end{equation}

\noindent To fit the VAE cost we rewrite the ELBO in terms of the conditional distribution of $x$ given $z$

\begin{align}
ELBO(q) = \langle \log(p(z)) \rangle +  \langle \log(p(x|z)) \rangle - \langle \log(q(z|x)) \rangle.
\end{align}

\noindent Finally the ELBO can be related to the VAE loss by applying Jensen's inequality (J) to the log evidence 

\begin{align}
\log (p(x)) &= \log \int_z p(x|z) p(z) \\
&= log \int _z p(x|z) p(z) \frac{q(z|x)}{q(z|x)} \\
& = log \langle p(x|z) p(z)/ q(z|x) \rangle \\
&  \stackrel{\mathclap{\text{(J)}}}{\geq} \langle \log (p(x|z) p(z)/ q(z|x))\rangle \\
& \geq \langle \log(p(x|z))\rangle + \langle \log(p(z))\rangle  - \langle \log(q(z|x))\rangle.
\end{align}

\noindent Which shows that the function enacted by the ELBO approximates the VAE loss up to a constant i.e. the KL loss on the RHS in equation \ref{eq:vae_cost}. \citet{Kingma2013} showed that this variational lower bound on the marginal likelihood of our data is feasibly approximated with a neural network when trained with backpropagation and gradient descent methods. That is we estimate the derivative of the ELBO with respect to the neural network parameters, as described by the backpropagation algorithm in section \ref{sec:backpropagation}. We note again that in the above notation we would parametrize the distribution $p(x|z)$ as a neural network, in machine learning parlance called the generator network and denoted as $\phi$ in section \ref{sec:autoencoder}. 

\subsection{Optimizing the variational autoencoder}

From section \ref{sec:autoencoder} we observe that the optimization is split in two. A reconstructive term that approximates the log evidence which we can train with a squared error or cross entropy cost. Secondly we have a divergence term over the parametrized and theoretical latent distribution. We would like to simplify the second to conserve computational resources. Thankfully this is simple given some assumptions on the target latent distribution. Let the target distribution $p(z | x) $ be a multivariate normal distribution with zero means and a diagonal unit variance matrix, i.e. $p(z | x) \sim  \mathcal{N}(\mathbf{0}, \mathbf{I})$. And accordingly the neural network approximation then follows $q(z | x) \sim \mathcal{N}(\mu, \Sigma)$. The normalized probability density function for the normal Gaussian is defined as 

\begin{equation}
p(x) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x- \mu)^T\Sigma^{-1}(x-\mu)),
\end{equation}

\noindent and the Kullback-Leibler divergence for two multivariate gaussians is given by 


\begin{equation}
\hspace*{-0.1in}
D_{KL}(p_1|| p_2 ) = \frac{1}{2} \left( \log \frac{|\Sigma_2|}{|\Sigma_1|} -n + tr(\Sigma^{-1}_2 \Sigma_1) + (\mu_2 - \mu_1)^T\Sigma_2^{-1}(\mu_2 - \mu_1) \right).
\end{equation}

\noindent Which we derive in appendix \ref{appendix:kl_gauss}. Substituting $p_1$ and $p_2$ for the model and target distributions we get 

\begin{align*}
D_{KL}(q||p) &= \frac{1}{2} \left( - \log {|\Sigma_1|} -n + tr(I \Sigma_1) + \mu_2 ^TI\mu_2 \right) \\
&= \frac{1}{2} \left( - \log {|\Sigma_1|} -n + tr(\Sigma_1) + \mu_2 ^T\mu_2 \right)
\end{align*}

\noindent or more conveniently

\begin{equation}\label{eq:kl_opt}
D_{KL}(q||p) = \frac{1}{2} \sum_i -\log \sigma_i^2 - 1 + \sigma^2_i + \mu_i^2 .
\end{equation}


\noindent We note that equation \ref{eq:kl_opt} satisfies the equality that the divergence is zero when the target and model distributions are equal. An important feature of the Kullback-Leibler divergence is that it operates point-wise on the probability densities. Which was the topic of the argument presented by \citet{Zhao} when proposing alternate measures for regularizing the latent space. The intuitive alternative to a point-wise measurement is comparing the moments of the distribution and minimize their difference. 