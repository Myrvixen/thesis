\section{Hyperparameters}\label{sec:hyperparams}

In fitting a model to data we adjust the parameters to fit some objective. However there are several parameters to the model that have to be heuristically determined that may impact performance without having a closed form derivative with respect to the optimization problem. These parameters are called hyperparameters and are vitally important to the optimization of machine learning models. In the simple linear or logistic regression case the hyperparameters include the choice of regularization norm (ordinarily the $L_1$ or $L_2$ norms) and the regularization strength $\lambda$ and the optimizer parameters like the learning rate $\eta$ and eventual momentum parameters $\beta_1, \beta_2, \dots$ the choices of all these parameters are highly non-trivial because their relationship can be strongly co- or contra-variant. Additionally the search for these parameters are expensive because each configuration of parameters is accompanied by a full training of the model. In this section we'll discuss the general process of tuning hyperparameters in general, and then we'll introduce specific parameters that need tuning in subsequent sections pertaining to particular architectures or modeling choices. Whichever scheme is chosen for the optimization they each follow the same basic procedure:

\begin{enumerate}
\item Choose hyperparameter configuration
\item Train model
\item Evaluate performance
\item Log performance and configuration
\end{enumerate}

\noindent When searching for hyperparameter configurations for a given model it becomes necessary to define a scale for the variable. Together with the range the scale defines the interval on which we do the search. That is the scale defines the interval width on the range of the parameter. Usually the scale is either linear or logarithmic, though some exceptions exist. As they are discussed for each model type or neural network cell type a suggested scale will also be discussed.

\subsection{Hand holding}
The most naive way of doing hyperparameter optimization is to manually tune the values by observing changes in performance metrics. Being naive and unfounded it is rarely used in modern modeling pipelines, outside the prototyping phase. For this thesis we use a hand held approach to get a rough understanding of the ranges of values over which to apply a more reasoned approach. 

\subsection{Grid Search}
Second on the ladder of naivet√© is the exhaustive search of hyperparameter configurations. This in this paradigm one defines a multi dimensional grid of parameters that is evaluated. If one has a set of $N$ magnitudes of the individual parameter sets $s = \{n_i\}_{i=0}^{N}$ with values of the individual parameters $\gamma_k$ and where $n_i = |\{\gamma_k\}|$ then the complexity of this search is $\mathcal{O}(\prod_{n \in s} n)$. For example in a linear regression we would want to find the optimal value for the learning rate $\eta = \{\eta_k\}$ and the regularization strength $\lambda = \{\lambda_k\}$ then this search is a double loop as illustrated in algorithm \ref{algo:gridsearch}. In practice however the grid search is rarely used as the computational complexity scales exponentially with the number and resolution of the parameters. The nested nature of the for loops is also extremely inefficient in the event that the hyperparameters are disconnected i.e. neither co- or contra-variant.  

\begin{algorithm}[H]
\SetAlgoLined
\KwData{Arrays of float values $\lambda$, $\eta$}
\KwResult{log of performance for each training}
Initialization ;\\
log $\leftarrow [\hspace{0.1in}]$;\\
\For{$\lambda_k \in \lambda$}{
	\For{$\eta_k \in \eta$}{
		opt $\leftarrow$ optimizer($ \eta _k $);\\
		model\_instance $\leftarrow$ model($\lambda _k$);\\
		metrics $\leftarrow$ model\_instance.fit($\mathbf{X}, \mathbf{\hat{y}}$, opt) ;\\
		log.append((metrics, ($\lambda_k$, $\eta_k$)))
	}
}
\caption{Showing a grid search hyperparameter optimization for two hyperparameters $\eta$ and $\lambda$}\label{algo:gridsearch}
\end{algorithm}


\subsection{Random Search}
Surprisingly the hyperparameter search heuristic that has proven to be among the most effective is a simple random search. \citet{Bergstra2012} showed empirically the inefficiency of doing grid search and proposed the simple remedy of doing randomized configurations of hyperparameters. The central argument of the paper is elegantly presented in figure \ref{fig:randomsearch}. The majority of hyperparameter searches in this thesis is done by way of random searched. 

\begin{figure}[H]
\centering
\hspace*{-1.3in}\includegraphics{../figures/randomsearch.png}
\caption{Showing the inefficiency of grid search. Since the optimum of the loss with respect to a hyperparameter might be very narrowly peaked a grid search might miss the optimum in it's entirety. A random search is less likely to make the same error as shown by \citet{Bergstra2012}. Figure copied from \citet{Bergstra2012}}\label{fig:randomsearch}
\end{figure} 
