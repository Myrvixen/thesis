% !TEX spellckeck=en_GB

\section{Linear Regression}\label{sec:LinReg}

Modern machine learning has its foundations in the familiar framework of linear regression. Many fairly interesting problems can be cast as systems of linear problems, and as such there are multitudes of ways to solve the problem. In this section we'll detail the derivation of linear regression in the formalism of a maximum likelihood estimate. As it is in this formalism the thesis writ large is cast. Linear regression can be expressed on a general form as the linear relationship expressed in equation \ref{eq:linreg} where we don't specify the basis of $\mathbf{w}$, but we are free to model using polynomial, sinusoidal or ordinary Cartesian basis-sets.

\begin{equation}\label{eq:linreg}
\hat{y}_i = \mathbf{x}_i^T\mathbf{w} + b
\end{equation}

\noindent In addition to equation \ref{eq:linreg} we introduce the error term $\epsilon_i= y_i - \hat{y_i}$ which is the difference between the models prediction, $\hat{y}_i$, and the actual value $y_i$. The goal of linear regression is to minimize this error, in mathematical terms we have an optimization problem on the form

\begin{equation}
\mathcal{O} = \argmin _{\mathbf{w}, b} || \mathbf{y} - \mathbf{Xw} ||^2
\end{equation}

\noindent  The central assumption of linear regression, that provides the opportunity for a closed form solution, is the independent identically distributed (IID) nature of $\epsilon_i$. We assume that the error is normally distributed with zero-mean and identical variance across all samples, e.g. 

\begin{align}
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
\end{align}

\noindent And similarly we consider the model predictions to be normally distributed, but with zero variance, e.g.

\begin{align}
\hat{y}_i \sim \mathcal{N}(\mathbf{x}_i^T\mathbf{w}, 0)
\end{align}

\noindent For simplicity we include the intercept term, $b$,  in $\mathbf{w}$ and extend the full data-matrix $\mathbf{x}$ with a column of ones to compensate. Where we repeat again that the normal distribution has a probability density function (PDF) defined as 

\begin{align}
p(x | \mu, \sigma) = \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x - \mu)^2}{\sigma^2}}
\end{align}

\noindent This allows us to consider the real outcomes $y_i$ as a set of normally distributed variables too. By the linearity of the expectation operator we have 

\begin{align}
\langle y \rangle &= \langle \hat{y} + \epsilon \rangle  \\
\langle y \rangle &= \langle \hat{y} \rangle + \langle \epsilon \rangle \\
\langle y \rangle &= \mathbf{x}^T\mathbf{w}
\end{align}

\noindent And by the exact same properties we have that the variance of the prediction is the variance of the error term 

\begin{align}
\langle y\rangle^2 + \langle y^2\rangle = \sigma^2
\end{align}

\noindent In concise terms we simply consider our outcome as a set of IID normal variables on the form $y_i \sim \mathcal{N}(\mathbf{x}^T\mathbf{w}, \sigma^2)$. The likelihood of the linear regression can then be written using the same tuple notation as for equation \ref{eq:likelihood}

\begin{align}
p(S|\mathbf{\theta}) &= \prod_i^n p(y_i) \\
&= \prod_i^n \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_i - \mathbf{x}^T\mathbf{w})^2}{\sigma^2}} \\
&= \left(\frac{1}{\sqrt{2\pi \sigma^2}} \right)^n \prod_i^n e^{-\frac{(y_i - \mathbf{x}^T\mathbf{w})^2}{\sigma^2}}
\end{align}

\noindent We recall from section \ref{sec:models} that the best parameters of a model can be estimated with 

\begin{equation}
\mathbf{\theta}^* = \argmax_\mathbf{\theta} p(S|\mathbf{\theta})
\end{equation}

\noindent To find the optimal values we then want to take the derivative w.r.t the parameters and find a saddle point, but as we saw before this is impractical, if not impossible, with the product sum in the likelihood. To solve this problem we repeat the log-trick from section \ref{sec:models} re-familiarizing ourselves with the log-likelihood
\begin{align}
\log(p(S|\theta)) = n \log(\frac{1}{\sqrt{2\pi \sigma^2}}) - \frac{(\mathbf{y} - \mathbf{X}\mathbf{w})^2}{\sigma^2}
\end{align}

\noindent Taking the derivative with respect to the model parameters and setting to zero we get

\begin{align*}
\nabla_\mathbf{w} \log(p(S|\theta)) &=\nabla_\mathbf{w}\left( - \frac{1}{\sigma^2} (\mathbf{y} - \mathbf{X}\mathbf{w})^2 \right) \\
&= -\frac{1}{\sigma^2} 2 \mathbf{X}^T(\mathbf{y}- \mathbf{X}\mathbf{w}) \\
0 & = -\frac{2}{\sigma^2}(\mathbf{X}^T\mathbf{y} - \mathbf{X}^T\mathbf{Xw})&& \text{setting derivative to zero} \\
\mathbf{X}^T\mathbf{Xw} &= \mathbf{X}^T\mathbf{y}  && \text{multiplying away constants} \\
\mathbf{w}&= (\mathbf{X}^T \mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} 
\end{align*}

\noindent Which is the same solution for the parameters as we get with ordinary least squares. This problem is of course solvable with a plethora of other tools, most notably we have the ones that don't perform the matrix inversion $(\mathbf{X}^T \mathbf{X})^{-1}$ as this derivative might not be well defined. 