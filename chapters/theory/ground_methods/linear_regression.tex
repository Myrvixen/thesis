% !TEX spellckeck=en_GB

\section{Linear Regression}\label{sec:LinReg}

Modern machine learning has its foundations in the familiar framework of linear regression. Many fairly interesting problems can be cast as systems of linear problems, and as such there are multitudes of ways to solve the problem. The most straight forward of which is with just a little bit of linear algebra.

We begin by defining the constituents of our model: let our data be denoted as a matrix $\mathbf{X} \in \mathcal{R}^{m\times n+1}$ where $m$ is the number of data-points and $n$ the number of features. The $+1$ factor in the dimension is to accommodate the addition of a column of ones to our data as a convenient placeholder for the model intercept. Note also that the term features is used broadly in machine learning literature and denotes the measurable aspects of our system; in the 1D ising model the $n$ would denote the number of spins and $m$ the number of measurements we made on that system. Furthermore let the parameter, or weight, matrix be given as $\mathbf{w} \in \mathcal{R}^{n + 1\times k}$. Generally the outcome-dimension $k$ can be greater than one when estimating a multi-variate outcome, we will however limit this section to the case where $k=1$. The linear regression model is then the transformation of the input using the weight matrix, i.e. 

\begin{equation}\label{eq:og_linreg}
\mathbf{y} = \mathbf{X}\mathbf{w}.
\end{equation}
\noindent Finally let the outcome we wish to approximate be given as a vector $\mathbf{\hat{y}} \in \mathcal{R}^m$. 

The challenge is then finding the weights that give correct predictions from data. To measure the quality of our model we introduce the squared distance between our predictions and $\mathbf{\hat{y}}$, which also gives us a path to optimization by the first derivative in terms of the model parameters. The squared error is defined in terms of the Euclidean vector norm  

$$L_2(\mathbf{x}) =||\mathbf{x}||_2 = \left(\sum x_i^2\right)^{\frac{1}{2}},$$

\noindent which we apply to the vector difference between model output and true values and take the square to give us a nice differentiable form. In mathematical terms we define the squared error objective as

\begin{equation}
\mathcal{O} = || \mathbf{\hat{y}} - \mathbf{y} ||_2 ^2.
\end{equation}

\noindent An objective function $\mathcal{O}$ defines some optimization problem to minimize or maximize. In machine learning these are mostly commonly cast as minimization problems. Objective functions that define an optimization problem for finding minima are known as cost functions. In this thesis we use the symbol $\mathcal{C}$ to indicate such functions and the optimization problem is then finding the minimum w.r.t the parameters $\theta^*$, i.e. 

\begin{align}\label{eq:cost}
\theta^* = \argmin_\theta \mathcal{C}(y_i, f(x_i; \theta)).
\end{align}

\noindent We use the starred notation to indicate the optimal parameters for that optimization.

Returning to the least squares problem the task is finding the optimal parameters now requires a differentiation, and to aid in that we write the objective as a matrix inner product 

\begin{align*}
\mathcal{O} &= || \mathbf{\hat{y}} - \mathbf{y} ||_2 ^2, \\
\mathcal{O} &= ( \mathbf{\hat{y}} - \mathbf{Xw})^T( \mathbf{\hat{y}} - \mathbf{Xw}).
\end{align*}

\noindent Since the problem is one of minimization we take the derivative with respect to the model parameters and locate its minima by setting it to zero 

\begin{align}
\nabla _{\mathbf{w}}\mathcal{O} &= \nabla _{\mathbf{w}} ( \mathbf{\hat{y}} - \mathbf{Xw})^T( \mathbf{\hat{y}} - \mathbf{Xw}), \\
&= -2\mathbf{X}^T\mathbf{\hat{y}} + 2\mathbf{X}^T\mathbf{X}\mathbf{w}, \\
\mathbf{0} &= -2\mathbf{X}^T\mathbf{\hat{y}} + 2\mathbf{X}^T\mathbf{X}\mathbf{w}, \\
\mathbf{X}^T\mathbf{\hat{y}} &= \mathbf{X}^T\mathbf{X}\mathbf{w}, \\
\mathbf{w} &=(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{\hat{y}}. \label{eq:least_squares}
\end{align}

 \noindent This problem is of course solvable with a plethora of other tools, most notably we have the ones that don't perform the matrix inversion $(\mathbf{X}^T \mathbf{X})^{-1}$ as this inverse is not unique for data that does not have full rank.

Admittedly the least squares linear regression model is fairly simple and is rarely applicable to complex systems. Additionally we've not mentioned the assumptions made to ensure the validity of the model, the most important of which concerns the measurement errors which should be independent identical normally distributed. But given it's relative simplicity and the fact that an analytical solution can be found we'll use it for reference in understanding the following sections.




