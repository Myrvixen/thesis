\subsection{Regularization}

With the advent of modern computing resources researchers gained the ability to operate very complex models. Giving rise to the problem of overfitting to noise in the data, as described in section \ref{sec:fitting}. Finding measures to reduce overfitting has been a goal for near on $50$ years. The first modern breakthrough was adding a coefficient restriction on their cumulative magnitude by the $L_2$-norm. This form of restriction proved hugely beneficial for the simple reason that it restricted the models ability to express all of its complexity. Introduced in 1970 by \citet{Hoerl1970} this modification on linear regression was dubbed \textit{ridge} regression. Experiments with different norms were carried out in the years following the elegant discovery by \cite{Hoerl1970}. Perhaps most influential of them is the use of the $L_1$-norm first successfully implemented by \citet{Tibshirani1996}. The inclusion of a penalty by the $L_1$-norm proved challenging as it was not solvable analytically and required iterative methods like gradient descent, described in detail in section \ref{sec:gd}. 

In this section we will introduce the different regularizations as additional contributions to the cost function. As well as geometrically exploring the reasoning for why these terms reduce the expressibility of the model. The $L_p$ norm is defined as. 

\begin{equation}
L_p(\mathbf{x}) = \left(\sum |x_i|^p\right)^{\frac{1}{p}}
\end{equation}

\noindent A common notation for the $L_p(\cdot)$ norm that we will also use in this thesis is $L_p(\cdot) = ||\cdot||_p$. We note that the familiar euclidian distance is just the $L_2$ norm of a vector difference. The squared $L_2$ norm was added to form ridge regression (\cite{Hoerl1970}). For Lasso (\cite{Tibshirani1996}) the authors used an $L_1$ term, commonly called the Manhattan or taxicab-distance. The Manhattan distance is aptly named as one can think of it as the length of the city blocks a cab-driver drives from one house to another.

Modifying the cost function then is as simple as adding the normed coefficients. To demonstrate we add the ridge regularization term to the squared error cost, shown in equation \ref{eq:mse_ridge}

\begin{equation}\label{eq:mse_ridge}
C(y_i, f(\mathbf{x}_i; \theta)) = (y_i - f(\mathbf{x}_i; \theta))^2 + \sum|\theta_i|^2
\end{equation}

\noindent Conceptually the regularization term added to the cost function modifies what parameters satisfy the $\argmin$ in equation \ref{eq:cost} by adding a penalty to parameters having high values. This is especially useful in cases where features are co-variate or the data is noisy. Adding a regularization term is equivalent to solving a constrained optimization problem e.g. 

\begin{align}
\theta^* = \argmin_{||\theta||_2^2<t} ||y_i - f(\mathbf{x}_i; \theta)||_2^2
\end{align}

\noindent The representation as a constrained optimization is useful to understand the impact of this form of regularization. Graphically this is demonstrated in figure \ref{fig:regularization}, copied from \cite{Mehta2019}, where the lasso penalty is shown to result in a constrained region for the parameter inside a region with vertices pointing along the feature axes. Intuitively this indicates that for a $L_1$ penalty the optimal solution is a sparse one where as many parameters as possible are zero while still minimizing the squared error, or cross entropy. For $L_2$ ridge regression these vertices are not present and the region has an even boundary along the feature axes resulting in solutions where most parameter values are small. \todo{add subsection on dropout and batchnorm} 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../figures/regularization}
\caption{Demonstrating the effect of a regularization constraint on a 2-variable optimization. The blue ovals represent the squared error, as it is quadratic in the parameters $w_i$. And the shaded brown region represents the restriction on the values of $w_i$, s.t. the only eligible values for the parameters are inside this region. Since the $L_1$ norm has these vertices on the feature axis we expect that the contour of the cost will touch a vertex consequently generating a sparse feature representation. The $L_2$ norm does not have these protrusions and will then generally intersect with the cost-contour somewhere that generates a linear combination of features that all have small coefficients. Figure copied from \citet{Mehta2019}, which in turn adapted a figure from Friedman et al. (2001)}\label{fig:regularization}
\end{figure}