\subsection{Regularization}\label{sec:regularization}

With the advent of modern computing resources researchers gained the ability to operate very complex models which gives rise to the problem of overfitting. Overfitting describes a process where the model strongly fits to the noise part of the data and to a lesser degree the signal. With the consequence that while performance on the data the model is fit on increases it rapidly deteriorates outside that region. As much of current research deals with somehow bridging the barriers between different regions of data, or entirely different distributions, reducing the ability for a model to overfit is crucial in most applications. 

Finding measures to reduce overfitting has been a goal for machine learning researchers for near on $50$ years. The first modern breakthrough was adding a constraint on the coefficients cumulative magnitude. The first successful implementation used a threshold on the squared $L_2$-norm to limit coefficients. This form of restriction proved hugely beneficial for the simple reason that it restricted the models ability to express all of its complexity. Introduced in 1970 by \citet{Hoerl1970} the addition of a norm-constraint to linear regression was dubbed \textit{ridge} regression. Experiments with different norms were carried out in the years following the elegant discovery by \cite{Hoerl1970}. Perhaps most influential of them is the use of the $L_1$-norm, first successfully implemented by \citet{Tibshirani1996}. As the norms have different geometric expressions the consequence of their addition was evident in the types of solutions generated by their inclusion. The inclusion of an $L_1$-norm to the linear regression cost-function proved to be challenging as had no closed form solution and thus required iterative methods like gradient descent, described in detail in section \ref{sec:gd}. 

In this section we will introduce the different regularizations as additional contributions to the cost-function. As well as a brief exploration of their different geometrical shapes which provides the reasoning for why, and in what manner, these terms reduce the expressibility of the model. We begin with the general $L_p$ norm, which is defined as

\begin{equation}
L_p(\mathbf{x}) = \left(\sum |x_i|^p\right)^{\frac{1}{p}}.
\end{equation}

\noindent A common notation for the $L_p(\cdot)$ norm that we will also use in this thesis is $L_p(\cdot) = ||\cdot||_p$. We note that the familiar euclidian distance is just the $L_2$ norm of a vector difference. Wile the $L_1$ term is commonly called the Manhattan or taxicab-distance. The Manhattan distance is aptly named as one can think of it as the length of the city blocks a cab-driver drives from one house to another.

Modifying the cost function then is as simple as adding the normed coefficients. To demonstrate we add a ridge regularization term to the squared error cost

\begin{equation}\label{eq:mse_ridge}
C(y_i, f(\mathbf{x}_i; \theta)) = (y_i - f(\mathbf{x}_i; \theta))^2 + \sum|\theta_i|^2.
\end{equation}

\noindent Conceptually the regularization term added to the cost function modifies what parameters satisfy the $\argmin$ in equation \ref{eq:cost} by adding a penalty to parameters having high values. This is especially useful in cases where features are co-variate or the data is noisy. Adding a regularization term is equivalent to solving a constrained optimization problem e.g. 

\begin{align}
\theta^* = \argmin_{||\theta||_2^2<t} ||y_i - f(\mathbf{x}_i; \theta)||_2^2
\end{align}

\noindent The representation as a constrained optimization is useful to understand the impact of this form of regularization. The geometrical interpretation of this is shown in figure \ref{fig:regularization}, copied from \cite{Mehta2019}, where the lasso penalty is shown to result in a constrained region for the parameter inside a region with vertices pointing along the feature axes. Intuitively this indicates that for a $L_1$ penalty the optimal solution is a sparse one where as many parameters as possible are zero while still minimizing the squared error, or cross entropy. For $L_2$ ridge regression these vertices are not present and the region has an even boundary along the feature axes resulting in solutions where most parameter values are small. 

Regularization then reduces the probability of overfitting by limiting the expressed complexity of a model. In the example of polynomial regression lasso regularization forces many of the coefficients to be zero-valued in such a way that it still performs maximally. \todo{add subsection on dropout and batchnorm} 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../figures/regularization}
\caption[Geometric interpretation of the $L_1$ and $L_2$ regularization and the squared error cost]{Demonstrating the effect of a regularization constraint on a 2-variable optimization. The blue ovals represent the squared error, as it is quadratic in the parameters $w_i$. And the shaded brown region represents the restriction on the values of $w_i$, s.t. the only eligible values for the parameters are inside this region. Since the $L_1$ norm has these vertices on the feature axis we expect that the contour of the cost will touch a vertex consequently generating a sparse feature representation. The $L_2$ norm does not have these protrusions and will then generally intersect with the cost-contour somewhere that generates a linear combination of features that all have small coefficients. Figure copied from \citet{Mehta2019}, which in turn adapted a figure from Friedman et al. (2001)}\label{fig:regularization}
\end{figure}

\subsection{Batch Normalization}\label{sec:batchnorm}
\todo{maybe dropout?}