\section{Model Fitting}\label{sec:models}

The process of fitting models to data is the formal framework by which much of modern science is underpinned. In most sciences the researcher has a need to formulate some model that represents the theory at hand. In physics we construct models to describe complex natural phenomena by which we can make predictions or infer inherent properties of the system from. The models we use vary from simple linear models describing the nearest neighbor ising model in statistical mechanics to variational markov chain monte-carlo simulations for many-body quantum mechanics. Common for all these applications is the need to fit the model to the data at hand. The model would describe something general about systems similar the one under scrutiny and the fitting procedure is the way by which the model is tailored to the system at hand. 

In this thesis we consider a special case of model fitting commonly known as function approximation. Wherein an unknown function $\hat{f}(\mathbf{X}) = \hat{y}$ is approximated by an instance of a model $f_\theta(\mathbf{X}) = y$. Where we generally don't have a good ansatz for the form of $\hat{f}$. The subscript $\theta$ denotes the model parameters we can adjust to minimize the discrepancy, $g(|\hat{y} - y|)$, between our approximation and the true target values. An  example of the function $g$ is the mean squared error function used in many modeling applications. In this paradigm we have access to the outcomes of our process, $\hat{y}$, and the system states, $\mathbf{X}$. However this thesis deals largely with the problem of modeling when one only has access to the system states. The concepts, terminology and challenges inherent to the former are also ones we have to be mindful of in the latter.

Approximating functions with access to process outcomes starts with the separation of our data into two sets with zero intersection. This is done such that we are able to estimate the performance of our model in the real world. To elaborate the need for this separation we explore the concepts of overfitting and underfitting to the data this chapter, but first we introduce some simple tools and terminology from statistical learning theory and information theory that is used throughout this thesis.

\subsection{On information}

In information theory one considers the amount of chaos in in a process and how much one needs to know to characterize such a process. As we'll see this ties into concepts well known to physicists from statistical and thermal physics. As a quick refresher we re-state that processes that are more random possess more information in this formalism, i.e. a rolling die has more information than a spinning coin. We define the information of an event in the normal way 

\begin{equation}
I = -\log(p(x))
\end{equation} 

\noindent We usually wish to have knowledge of a system, however, obtained by the expectation over information. This expectation is called the entropy of the system and is defined in a familiar way as 

\begin{equation}
H(p(x)) = -\langle I(x) \rangle_{p(x)} = - \sum _x p(x)\log(p(x))
\end{equation}

\noindent Depending on the choice of base of the logarithm this functional has different names. Perhaps widest used is log base 2 know as the Shannon entropy; describing how many bits of information we need to fully describe the process underlying $p(x)$. In machine learning, or indeed may other applications of modeling, we wish to encode a process with a model. We can then measure the amount of bits (or other units of information) it takes to encode $x ~ p(x)$ with the model distribution $q_{\theta}(x)$. In this thesis we will in general use greek subscripted letters on distributions to denote models. This measure is called the cross-entropy and is defined as

\begin{equation}
H(p, q) = - \sum_x p(x)\log(q_\theta(x))
\end{equation}

\noindent Tying the cross entropy to model optimization requires a quantity to optimize. We define the maximum likelihood estimate (MLE) which represents the probability of seeing the data, i.e. the set of tuples $\eta_i = \{\mathbf{x}_i, y_i\}$, at hand given our model and parameters. Given the feature vectors with binary class labels $S = \{\eta_i\}$ the likelihood of our model is defined as 

\begin{equation}\label{eq:likelihood}
p(S | \theta) = \prod_i q_\theta(x_i)^{y_i} - (1-q_\theta(x_i))^{1-y_i}
\end{equation}

\noindent We want to maximize this functional with respect to the parameters $\theta$. The product sum is problematic in this regard as it's gradient is likely to vanish as the number of terms increase, to circumvent this we take the logarithm of the likelihood defining the log-likelihood. Optimizing the log-likelihood yields the same optimum as for the likelihood as the logarithmic function is monotonic. \footnote{it is trivial to show that for optimization purposes any monotonic function can be used, the logarithm turns out to be practical for handling the product sum and exponents.}
\begin{equation}
\mathcal{L}(\mathbf{x}, y, \theta) = \log(p(S | \theta)) = \sum_i y_i\log(q_\theta(x_i)) + (1-y_i)(q_\theta(x_i))
\end{equation}

\noindent Where we observe this is simply the cross-entropy for the binary case. The optimization problem is then 

\begin{equation}
\theta^* = \argmax_\theta \mathcal{L}(\mathbf{x}, y, \theta )
\end{equation}

\noindent This formulation of the MLE for binary classification can be extended to the case of simple regression where one shows the mean squared error is the functional to optimize for. Common to most applications in machine learning is the solution of these optimization problems by the use of gradient descent on the cost, usually simply defined as the negative loss. Gradient descent is discussed in some detail in section \ref{sec:gd}.

\subsection{Over and under-fitting}

When fitting an unknown function it is often not clear what complexity is suitable for the model. Additionally compounding this problem is the ever present threat of various noise signals and measurement errors present in the data. Further complicating the issue is the nature of machine learning problems: we're almost always interested in extrapolating to unseen regions of data, in the machine learning vernacular these are sets of data we call \textsc{test}-sets. Data used to fit the model is called  \textsc{train}-sets. To illustrate this we'll consider the case of the one dimensional problem of polynomial regression. We note that this section follows closely that of section 2 in \citet{Mehta2019}, we also refer to this paper for a more in-depth introduction to machine learning for physicists. The concepts of over and under-fitting go hand-in-hand with two other concepts we'll introduce in this chapter, regularization and the bias-variance relationship. Firstly however we'll briefly introduce the concepts in over and under-fitting models. This topic is strongly related to the concepts of complexity, and the Vapnik-Chervonenkis theory of statistical learning, the details of which are outside the scope of this thesis. We start by considering a process we wish to model that is on the form shown in equation \label{eq:target}

\begin{equation}\label{eq:target}
y_i = P(x_i) + \epsilon_i
\end{equation}

\noindent In equation \ref{eq:target} the $\epsilon_i$ term expresses a noise term at that point, and the function $P(\cdot)$ is the true process which we are interested in modeling but whose shape is hidden from us. It is important to note that for data with no noise, most of the problems and cautions we describe in this chapter do not apply, however measuring any physical phenomenon inherently carries with it some noise. We wish to model this principally unknown process expressed by $y_i$ by using a polynomial of degree $n$, let $P^n$ be the set of polynomials which we can construct a polynomial of degree $n$ to fit to the observation. The distinguishing features of overfitting are shown in figure \ref{fig:overfit} where we fit polynomials of varying degrees to data drawn from a true distribution following a first and third order polynomial respectively. In the figure we observe the higher order models fitting to spurious trends in the data we can attribute to the noise. The higher expressibility of the model then leads to it capturing features of the noise that increase accuracy in the training domain but we observe that this rapidly deteriorates outside the training region. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../figures/y_distr.png}
\caption{Polynomial regression of varying degrees on data drawn from a linear distribution on the left and a cubic distribution on the right. Models of varying complexity indicated by their basis $P^n$ are fit to the train data and evaluated on the test region. We observe that the higher order solutions follow what we observe to be  spurious-noise generating features in the data. This is what we call overfitting.}\label{fig:overfit}
\end{figure}

\subsubsection*{Regularization}

\begin{enumerate}
	\item Describe the fitting of functions to data
	\item Overfitting and underfitting and measures to prevent both
\end{enumerate}