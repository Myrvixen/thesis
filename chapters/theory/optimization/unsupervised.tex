\section{Unsupervised learning}

In section \ref{sec:models} we presumed the data was comprised of system states and measured outcomes of the set of states. For some applications we do not have measured outcomes of states and are left with only the data itself. In that case we often wish to extract information about the differences in the data from the data itself. Clustering is one such approach wherein similarities are measured between points of data and grouped based on respective similarities. The principle of clustering analysis is well illustrated by the K-means algorithm (\cite{Neyman1967}). Using the euclidean distance between each point in the data and centroids that represent each cluster. The simplest version initializes random cluster centroids and follow an update rule by computing the mean of all points belonging to that cluster. This procedure is repeated until convergence of the centroid locations. Of course this algorithm is dependent on the cluster initialization, and performs poorly in high-dimensional spaces where the euclidean distance looses its discriminatory power. 

The latter restriction can be addressed by representing the data in a much smaller dimensional space. How we chose this representation then becomes the primary challenge. A naive approach is to find the eigenvectors of the co-variance matrix and project the data along those with the highest corresponding eigenvalues. This method is called PCA (principal component analysis, \cite{Marsland2009}) and has been a staple in the machine learning community for decades. The immediately apparent problem with PCA is that it captures linear axes of variation, while the data might very well have non-linear relationships. 

Enter the neural network, and more specifically the autoencoder. The neural network architecture known as autoencoders describe models based on efficient information-bottlenecking of complex data by reconstruction. To enact an efficient information bottle-neck the auto-encoder enacts two non-linear maps. One from the input dimension to a much lower dimensional projection, that is the map $\psi: \mathcal{R}^n \rightarrow \mathcal{R}^m,\, n >> m$, which we call the encoder network. The second part of the model then maps from the lower dimensional projection to the original dimension, i.e. $\phi: \mathcal{R}^m \rightarrow \mathcal{R}^n$, and is known as the decoder. And the objective of this model is the reconstruction of the input which means the compression map $\psi$ is optimized to capture salient information about the data. This compressions are interesting because they have been shown to convey semantic information through the compression (\cite{Fertig}). In this thesis we will use autoencoder models to explore the semantic information when processing events from a nuclear physics experiment. We will explore both the case where no labeled data exists and where there exists some subset of labeled data. Autoencoders and how to encourage semantic information in the compressed representation is discussed in greater detail in section \ref{sec:autoencoder}