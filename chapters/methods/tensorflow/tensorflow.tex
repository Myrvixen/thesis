\section{TensorFlow}\label{sec:TensorFlow}

The numerical framework TensorFlow is a development for deep learning tasks developed by Google Brain starting in 2011 (\cite{tensorflow}). It implements a total pipeline for a very large variety of machine learning architectures. At the core of the library is the graph structure constructed during runtime. Built for iteration the update of tensor objects is statically determined in a manner close to traditional compiled languages. Python indexing and iteration in loops are notoriously slow but can be sped up considerably to the point where for modestly sized computations the gain of switching to a \lstinline{C} style language is negligible. This speed up is achieved largely by avoiding pythons built in iterables and loop structures where possible, relying instead on interfaces to optimized \lstinline{C} or \lstinline{C++} code for very efficient matrix operations.

Part of the efficiency loss that numpy sustains is another python peculiarity; for most operations (adding, multiplying, etc.) the default behavior of numpy is to return a new object according to the broadcasting rules of the input with inferred element types. This is obviously a costly behavior while very much pythonic in spirit. Later versions ( > 1.9.0 ) allow for more operations to define an output array destination. While this allows numpy to catch up somewhat in speed the TensorFlow address to this problem solves both the challenge of object allocation and the computation of gradients so emblematic of modern machine learning.

\subsection{The computational graph}

To understand the program flow of the later algorithm implementations we begin by introducing the fundamental concepts of TensorFlow code \footnote{The thesis code was written for the latest stable release of TensorFlow prior to the release of \lstinline{TF 2.0}. Some modules may have moved, changed name or have otherwise been altered. Most notably in the versions prior to \lstinline{TF 2.0} eager execution was not the default configuration and as such the trappings of the implementation includes the handling of session objects.}. The heart of which is the computational graph. A code snippet with an associated graph is included in figure \ref{fig:graph} showing a simple program that computes a weight transformation of some input with a bias. The cost is included as the bracketed ellipses and is choses specifically for the problem. When the forward pass is unrolled it becomes available for automatic differentiation. 

\begin{figure}[H]
\centering
\lstinputlisting[language=iPython]{../snippets/tf_graph.py}
\caption[FCN forward pass in TensorFlow]{This short script describes the setup required to compute a forward pass in a neural network as described in section \ref{sec:ANN}. Including more layers is as simple as a for loop and TensorFlow provides ample variations in both cell choices (RNN variations, convolutional cells etc.) and activation functions. This script is a modified version of figure 1 in \cite{tensorflow}}\label{fig:graph}
\end{figure}
\begin{figure}
\includegraphics[height=1.2in]{../../../placeholder.png}
\caption[Graph representation of the forward pass of a simple FCN]{A graph representation of the short script in figure \ref{fig:graph}. The nodes represent TensorFlow operation types including variable declarations and operations on those including \lstinline{Add} and \lstinline{MatMul} operations. In the versions of TensorFlow prior to the release of TF 2.0 this graph did not execute immediately but relied on the session object. The \lstinline{Session.run} method takes as arguments input to the graph and the end point(s) and then computes the transitive closure of all the nodes that must be executed in order to obtain said output(s) (\cite{tensorflow}). Using tensorfow involves setting up a graph once and executing it or a few distinct subgraphs hundreds of thousands of times. This figure is a modified version of figure 2 in \citet{tensorflow}}
\end{figure}

In general we set up the computational graph to represent the forward pass, or predictive path, of the algorithm. The remainder then is then only to compute the gradients required to perform gradient descent. TensorFlow provides direct access to find the gradients via \lstinline{tf.gradients(C, [I]k)} where \lstinline{[I]k} represents the set of tensors we wish to find the gradients of C with respect to. The process of automatic differentiation we describe in detail in section \ref{sec:ANN}. But in essence the method finds the path on the graph from I to C and then works backwards, adding to the graph as it goes, computing the partial derivatives via the chain rule. We show the gist of this process in figure \ref{fig:grad_graph}. Since the operations on the partial derivatives are defined by the choice of gradient descent variation TensorFlow wraps the computation in optimizer modules for convenience. Defined in \lstinline{tf.train} these include stochastic gradient descent and  ADAM \todo{whats that damn abbreviation?}. 
 
\begin{figure}[H]
\centering
\includegraphics[height=1.2in]{../../../placeholder.png}
\caption{}\label{fig:grad_graph}
\end{figure}

We outline a basic TensorFlow script in \ref{fig:tf_script} that goes through the basic steps outlined above. This is the skeleton on which we build the more complex architecture for the DRAW algorithm.

\begin{figure}[H] 
\centering
\lstinputlisting[language=iPython]{../snippets/tf_script.py}
\caption[Computing gradients and performing back-propagation in TensorFlow]{A TensorFlow script that uses the \lstinline{tf.train} module to compute the gradients needed to perform backpropagation of errors on the cost function assigned to the variable \lstinline{C}. Additionally we show the structure of a session and it's run method to perform a backwards pass with respect to the loss \lstinline{C}}\label{code:tf_script}
\end{figure}
