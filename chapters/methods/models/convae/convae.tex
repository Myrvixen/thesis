
\subsection{Convolutional Autoencoder}\label{sec:convae_implement}

The convolutional autoencoder class \lstinline{ConVae} is implemented as a subclass of \lstinline{LatentModel}. It implements the construction of the computational graph and the compute operations needed for the available losses. The graph is manipulated with respect to a supplied configuration dictionary. Which includes options for additional regularization terms like batch-normalization and instructs the class on what losses to compile the model with. 

\subsubsection{Computational graph}

The private\footnote{The term private is used loosely in the context of python as the language does not actually maintain private methods unaccessible to the outside. By convention methods that are prefixed with an underscore are to be treated as private and are not exposed with public apis and in documentation.} function which enacts the computational graph is \lstinline{_ModelGraph}. It accepts arguments for the strength and type of regularization on the kernel and bias parameters as well as the activation function to be used for the internal representations and the projection to output space. 

Inside the method the placeholder variable \lstinline{ConVae.x} is defined. The placeholder defines the entry point of the forward pass and is where tensorflow allocates the batched data in the training operation. Depending on whether the model is instructed to use the VGG16 representation of the data or a specified encoder structure it applies dense weight transformations with non-linearities or runs a series of convolutional layers, respectively. Each convolutional layer is specified with a kernel size, a certain number of filters and the striding of the convolutions. We also use a trick from \citet{Guo2017} to ensure that the padding is chosen such that the reconstruction is unambiguous. 

The padding is set to preserve the input dimensionality and is only reduced in dimensionality with striding or max-pooling. Depending on whether the output width(height) is an integer multiple of $2^n$, where $n$ is the number of layers, the last convolution is adjusted to have no zero-padding if this is the case. 

After each layer the specified non-linearity is applied. This is one of the sigmoid activations (logistic sigmoid or hyperbolic tangent) or the rectified linear unit family of activations\footnote{The model accepts a \lstinline{None} argument for the activation in practice for debugging but this is not used fro any models in this thesis.}. If the model configuration specifies to use batch normalization this is applied before sigmoid functions and after rectified units. The reason for different points of application relates to the challenges of the respective activation families; sigmoids' saturate and so the input should be scaled and rectified units explode so the output is scaled. The output from the convolutional layers is then an object with dimensions \lstinline{h = (o, o, f)} where \lstinline{f} is the number of filters in the last layer and \lstinline{o} $= \frac{H}{2^n}$ with $n$ denoting the number of layers with stride $2$ or the count of \lstinline{MaxPool} layers and $H$ the input image size.

The tensor output from the convolutional layers is then transformed to the latent space with either a simple dense transformation, e.g. \lstinline{z = Dense(flatten(h))}. Or if a variational loss is specified a pair mean and standard deviation tensors is constructed with dense transformations from \lstinline{h}. Using the re-parametrization trick shown by \citet{Kingma2013} a sample is generated by \lstinline{z = mu + sigma*epsilon} where epsilon is a stochastic tensor from the multivariate uniform normal distribution, $\mathcal{N}(0, 1)$. The mean and standard deviation tensors are stored to be used in the computation of the loss. The latent sample is also stored for later retrieval or the computation of non-variational costs. 

After a sample \lstinline{z} is drawn the reconstruction is computed with either a mirrored decoder for the naive autoencoder structure or for a VGG16 representation of the data a reconstruction is computed based on a specified decoder structure. The VGG16 representation has the same call structure, but with a boolean flag to the model \lstinline{use_vgg} that indicates that the configuration is explicitly for the decoder. 

Finally, after the decoding from the latent sample, the output is passed through a sigmoid function if the reconstruction loss is specified as a binary cross-entropy to ensure that the log-term doesn't blow up. Otherwise no transformation is applied on the output. 

\subsubsection{Computing losses}

In the configuration dictionary the loss for both the reconstruction and latent spaces is specified. For the reconstruction the model accepts either a mean squared error or binary cross-entropy loss, the cross-entropy is the default. 

Each of these losses acts pixel-wise on the output and target images. The reconstruction loss is then stored as a class attribute \lstinline{ConVae.Lx} which is monitored by the \lstinline{TensorFlow} module \lstinline{TensorBoard} for easy monitoring during training. Depending on the configuration the model then compiles a loss over the latent space. For a variational autoencoder the loss is a Kullback-Leibler divergence over the latent distribution and a multivariate normal distribution with zero mean and unit variance. This has a closed form solution given a tensor representing the mean and standard deviation which we derived in equation \ref{eq:kl_opt}. This equation is relatively straightforward to implement as it just implies a sum over the mean and standard deviation tensor. The form of equation \ref{eq:kl_opt} also makes clear why we parametrize the standard deviation and not the variance directly as the exponentiation ensures positivity of the variance. 

Alternatively to a Kullback-Leibler divergence the latent space may be regularized in the style proposed by \citet{Zhao}. Which measures the  We use the radial basis function kernel to compute the maximum mean discrepancy divergence term introduced in equation \ref{eq:mmd} \todo{write implement of KLD and MMD?}


